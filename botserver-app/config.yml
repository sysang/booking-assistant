# The config recipe.
# https://rasa.com/docs/rasa/model-configuration/
# recipe: default.v1

# Configuration for Rasa NLU.
# https://rasa.com/docs/rasa/nlu/components/
version: "3.1"

language: en

importers:
- name: MultiProjectImporter

imports:
  - data/Intents
  - data/Entities
  - data/Slots
  - data/Actions
  - data/Forms

pipeline:
# No configuration for the NLU pipeline was provided. The following default pipeline was used to train your model.
# If you'd like to customize it, uncomment and adjust the pipeline.
# See https://rasa.com/docs/rasa/tuning-your-model for more information.
  - name: WhitespaceTokenizer
    intent_tokenization_flag: True
    intent_split_symbol: "+"
  - name: LanguageModelFeaturizer
    # Name of the language model to use
    model_name: "bert"
    # Pre-Trained weights to be loaded
    model_weights: "rasa/LaBSE"

    # An optional path to a directory from which
    # to load pre-trained model weights.
    # If the requested model is not found in the
    # directory, it will be downloaded and
    # cached in this directory for future use.
    # The default value of `cache_dir` can be
    # set using the environment variable
    # `TRANSFORMERS_CACHE`, as per the
    # Transformers library.
    cache_dir: null
  - name: RegexFeaturizer
  - name: LexicalSyntacticFeaturizer
  - name: CountVectorsFeaturizer
  - name: CountVectorsFeaturizer
    analyzer: char_wb
    min_ngram: 1
    max_ngram: 4
  - name: DIETClassifier
    intent_classification: True
    entity_recognition: True
    epochs: 51
    learning_rate: 0.001
    batch_size: [64, 256]
    constrain_similarities: true
    hidden_layers_sizes:
      text: [512, 128]
    dense_dimension:
      text: 128
      label: 32
    concat_dimension:
      text: 128
      label: 32
    connection_density: 0.3
    number_of_transformer_layers: 2
    transformer_size: 256
    number_of_attention_heads: 4
    use_masked_language_model: True
    embedding_dimension: 32
    drop_rate: 0.25  # Dropout rate for encoder.
    drop_rate_attention: 0.0  # Dropout rate for attention.
  - name: EntitySynonymMapper
  - name: ResponseSelector
    epochs: 100
    constrain_similarities: true
  - name: FallbackClassifier
    threshold: 0.15
    ambiguity_threshold: 0.12

# Configuration for Rasa Core.
# https://rasa.com/docs/rasa/core/policies/
policies:
# No configuration for policies was provided. The following default policies were used to train your model.
# If you'd like to customize them, uncomment and adjust the policies.
# See https://rasa.com/docs/rasa/policies for more information.
  # - name: MemoizationPolicy
  - name: RulePolicy
    core_fallback_threshold: 0.10
    core_fallback_action_name: custom_action_fallback
    enable_fallback_prediction: True
    restrict_rules: true
    check_for_contradictions: true
  - name: UnexpecTEDIntentPolicy
    tolerance: 0.65
    max_history: 4
    learning_rate: 2e-3
    epochs: 201
    constrain_similarities: true
    model_confidence: softmax  # softmax | linear_norm
    batch_size: [64, 256]
    connection_density: 0.85
    dense_dimension:
      text: 16
      action_text: 16
      label_action_text: 16
      intent: 32
      action_name: 16
      label_action_name: 16
      entities: 16
      slots: 32
      active_loop: 16
    concat_dimension:
      text: 64
      action_text: 64
      label_action_text: 64
    encoding_dimension: 64
    transformer_size:
      text: 128
      dialogue: 128
      action_text: 128
      label_action_text: 128
    number_of_transformer_layers:
      text: 3
      dialogue: 3
      action_text: 3
      label_action_text: 3
    number_of_attention_heads: 16
    embedding_dimension: 32
  - name: TEDPolicy
    max_history: 2
    learning_rate: 2e-3
    epochs: 1234
    constrain_similarities: true
    model_confidence: softmax
    batch_size: [64, 256]
    connection_density: 0.85
    dense_dimension:
      text: 16
      action_text: 16
      label_action_text: 16
      intent: 32
      action_name: 16
      label_action_name: 16
      entities: 16
      slots: 32
      active_loop: 16
    concat_dimension:
      text: 64
      action_text: 64
      label_action_text: 64
    encoding_dimension: 64
    transformer_size:
      text: 128
      dialogue: 128
      action_text: 128
      label_action_text: 128
    number_of_transformer_layers:
      text: 3
      dialogue: 3
      action_text: 3
      label_action_text: 3
    number_of_attention_heads: 16
    embedding_dimension: 32
