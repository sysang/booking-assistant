# The config recipe.
# https://rasa.com/docs/rasa/model-configuration/
# recipe: default.v1

# Configuration for Rasa NLU.
# https://rasa.com/docs/rasa/nlu/components/
version: "3.1"

language: en

importers:
- name: MultiProjectImporter

imports:
  - data/Intents
  - data/Entities
  - data/Slots
  - data/Actions
  - data/Forms

pipeline:
# No configuration for the NLU pipeline was provided. The following default pipeline was used to train your model.
# If you'd like to customize it, uncomment and adjust the pipeline.
# See https://rasa.com/docs/rasa/tuning-your-model for more information.
  - name: WhitespaceTokenizer
    intent_tokenization_flag: True
    intent_split_symbol: "+"
  - name: LanguageModelFeaturizer
    # Name of the language model to use
    model_name: "bert"
    # Pre-Trained weights to be loaded
    model_weights: "rasa/LaBSE"

    # An optional path to a directory from which
    # to load pre-trained model weights.
    # If the requested model is not found in the
    # directory, it will be downloaded and
    # cached in this directory for future use.
    # The default value of `cache_dir` can be
    # set using the environment variable
    # `TRANSFORMERS_CACHE`, as per the
    # Transformers library.
    cache_dir: null
  - name: RegexFeaturizer
  - name: LexicalSyntacticFeaturizer
  - name: CountVectorsFeaturizer
  - name: CountVectorsFeaturizer
    analyzer: char_wb
    min_ngram: 1
    max_ngram: 4
  - name: DIETClassifier
    intent_classification: True
    entity_recognition: True
    epochs: 301
    learning_rate: 2e-4
    batch_size: [32, 512]  # default [64, 256]
    constrain_similarities: true
    hidden_layers_sizes:
      text: [512, 128]
    dense_dimension:
      text: 192
      label: 32
    concat_dimension:
      text: 192
      label: 32
    connection_density: 0.15
    transformer_size: 384  # default 256
    number_of_transformer_layers: 2
    number_of_attention_heads: 4
    use_masked_language_model: True
    embedding_dimension: 32
    drop_rate: 0.25  # Dropout rate for encoder. Default: 0.2
    drop_rate_attention: 0.0  # Dropout rate for attention.
    number_of_negative_examples: 50  # default: 20
    similarity_type: auto  # default: auto
  - name: EntitySynonymMapper
  - name: ResponseSelector
    epochs: 100
    constrain_similarities: true
  - name: FallbackClassifier
    threshold: 0.25
    ambiguity_threshold: 0.15

# Configuration for Rasa Core.
# https://rasa.com/docs/rasa/core/policies/
policies:
# No configuration for policies was provided. The following default policies were used to train your model.
# If you'd like to customize them, uncomment and adjust the policies.
# See https://rasa.com/docs/rasa/policies for more information.
  # - name: MemoizationPolicy
  - name: RulePolicy
    core_fallback_threshold: 0.10
    core_fallback_action_name: custom_action_fallback
    enable_fallback_prediction: True
    restrict_rules: true
    check_for_contradictions: true
  - name: UnexpecTEDIntentPolicy
    tolerance: 0.43
    max_history: 3
    learning_rate: 1e-3
    epochs: 301
    constrain_similarities: true
    model_confidence: softmax
    batch_size: [32, 256]
    connection_density: 0.2
    dense_dimension:
      text: 128
      intent: 20
      action_name: 20
      label_intent: 20
      entities: 20
      slots: 20
      active_loop: 20
    concat_dimension:
      text: 128
      action_text: 128
      label_action_text: 128
    encoding_dimension: 50
    transformer_size:
      text: 128
      dialogue: 128
      action_text: 128
      label_action_text: 128
    number_of_transformer_layers:
      text: 1
      dialogue: 1
      action_text: 1
      label_action_text: 1
    number_of_attention_heads: 4
    embedding_dimension: 20
  - name: TEDPolicy
    max_history: 2
    learning_rate: 1e-3  # default 1e-3
    epochs: 1023
    constrain_similarities: true
    model_confidence: softmax
    batch_size: [32, 256]  # default [32, 256]
    connection_density: 0.2  # default 0.2
    dense_dimension:
      text: 128  # default 128
      intent: 20  # default 20
      action_name: 20
      label_intent: 20
      entities: 20
      slots: 20
      active_loop: 20
    concat_dimension:
      text: 128
      action_text: 128
      label_action_text: 128  # default 128
    encoding_dimension: 50
    transformer_size:
      text: 128
      dialogue: 128
      action_text: 128
      label_action_text: 128
    number_of_transformer_layers:
      text: 1
      dialogue: 1
      action_text: 1
      label_action_text: 1  # default 1
    number_of_attention_heads: 4
    embedding_dimension: 20  # default 20
